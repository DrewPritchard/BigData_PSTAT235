{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe assembler\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "* Input original dataset\n",
    "* Conduct feature engineering for the following columns:\n",
    "    * Lat/long: Add clusters, potentially; also neighborhood/other vars\n",
    "    * Features: Exploded? and k-means clustering into 20 clusters -- kapow!\n",
    "    * Manager: Add a manager score\n",
    "    * Description: replace with text analysis thing, add columns for exclamations and punctuation\n",
    "* This will generate a dataframe with several 'features' columns (eg. 'features_description', 'features_manager' etc.)\n",
    "* We will then combine these columns into a single column of features vectors:\n",
    "https://scikit-learn.org/0.18/auto_examples/hetero_feature_union.html looks very helpful for doing this\n",
    "\n",
    "* We then split the data using 20% testing, 80% cv with 5 folds of 16% to parameterize the model\n",
    "\n",
    "    * First model= logistic regression using no engineered features\n",
    "    * Second model= random forest with no engineered features\n",
    "\n",
    "    * Third model= logistic regression with engineered features\n",
    "    * Fourth model= random forest with engineered features\n",
    "\n",
    "\n",
    "Cross-validation and model comparison is based on log-loss.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Assemble_Data\") \\\n",
    "    .config(\"spark.executor.memory\", '4g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data_pd = pd.read_json(\"data/train.json\")\n",
    "train_data_df = sqlCtx.createDataFrame(train_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Lat/long work:\n",
    "\n",
    "**Still need to implement pipeline. I had a little trouble with it but will add it soon.**\n",
    "\n",
    "Also, the clusters are mapped to each observaition via a unique id that I added but the id is not returned. Not sure if this causes problems or not but if it does we'll need to add the ID to the train df prior to adding the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer #For grouping lat and long into buckets for clustering\n",
    "from pyspark.ml.clustering import KMeans #Clustering coord data\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer #index response variable as numerical\n",
    "from pyspark.sql.functions import monotonically_increasing_id #Joining clusters back to original data need a unique ID\n",
    "import numpy #Create splits vector for bucketizing\n",
    "\n",
    "#Make sure to clean lat and long prior to this. Or I could put the cleaning in the function.\n",
    "#I'm not sure how to use xining's outlier smoother function and I used a different method in\n",
    "# my code. \n",
    "def get_coord_clusters(df, inputCol1, inputCol2, numClust):\n",
    "    #Get df of only columns of interest\n",
    "    partial_df = df.select(inputCol1, inputCol2, 'interest_level')\n",
    "    \n",
    "    ###Index interest level(Probably will need to be removed. Not necessary and will be done\n",
    "    ###in other parts of code.)\n",
    "    \n",
    "    #Instantiate and fit indexer and drop old interest_level\n",
    "    indexer = StringIndexer(inputCol = 'interest_level', outputCol = 'interest_level_index')\n",
    "    partial_df = indexer.fit(partial_df).transform(partial_df).drop('interest_level')\n",
    "    \n",
    "    ###Bucketize latitude and longitude: based on the min and max of their range.\n",
    "    \n",
    "    #Get min and max of each col\n",
    "    min1 = partial_df.agg({inputCol1: 'min'}).collect()[0][0]\n",
    "    max1 = partial_df.agg({inputCol1: 'max'}).collect()[0][0]\n",
    "    \n",
    "    min2 = partial_df.agg({inputCol2: 'min'}).collect()[0][0]\n",
    "    max2 = partial_df.agg({inputCol2: 'max'}).collect()[0][0]\n",
    "    \n",
    "    #Get splits\n",
    "    splits1 = numpy.arange(min1 - .001, max1 + .001, .001) #widen interval by .0001 to make sure all values are bucketized\n",
    "    splits2 = numpy.arange(min2 - .001, max2 + .001, .001)\n",
    "    #Instantiate bucketizers\n",
    "    bucketizer1 = Bucketizer(splits = splits1, inputCol = inputCol1, outputCol = 'bucketed1')\n",
    "    bucketizer2 = Bucketizer(splits = splits2, inputCol = inputCol2, outputCol = 'bucketed2')\n",
    "    #Bucketize both columns and add them to our df\n",
    "    partial_df = bucketizer1.transform(partial_df)\n",
    "    partial_df = bucketizer2.transform(partial_df)\n",
    "    \n",
    "    ###Vectorize our buckets\n",
    "    \n",
    "    # This will return a new DF with all the columns + unique id\n",
    "    partial_df = partial_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "    #Create('assemble') a vector of longitude and latitude to feed into our model as\n",
    "    # a single predictor variable\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols = ['bucketed1', 'bucketed2'],\n",
    "        outputCol = 'featuresVec')\n",
    "\n",
    "    #Run our df through the assembler\n",
    "    partial_df = assembler.transform(partial_df)\n",
    "\n",
    "    \n",
    "    ###KMeans Clustering\n",
    "    \n",
    "    #Instantiate\n",
    "    kmeans = KMeans().setK(numClust).setFeaturesCol('featuresVec')\n",
    "    model = kmeans.fit(partial_df)\n",
    "    \n",
    "    #Get clusters and join them with partial_df\n",
    "    transformed = model.transform(partial_df).select('prediction', 'id')\n",
    "    partial_df = transformed.join(partial_df, 'id')\n",
    "    \n",
    "    #Rename predictions\n",
    "    partial_df = partial_df.withColumnRenamed('prediction', 'CoordinateCluster')\n",
    "\n",
    "    \n",
    "    return partial_df['CoordinateCluster']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Features' work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have commented the rows, for more thurough explanations look in `rentalPrice_jonas.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "def get_features_column(df, inputCol = \"features\"):\n",
    "    \n",
    "    #creates 1 sting of the features\n",
    "    string_assembler = F.UserDefinedFunction(lambda x: ','.join(x), typ.StringType())\n",
    "    df = df.withColumn(inputCol, string_assembler(df[inputCol]))\n",
    "    #lower case everything\n",
    "    df = df.withColumn(inputCol, F.lower(df[inputCol]))\n",
    "    #adds feature \"missing features\" to NaN\n",
    "    df = df.withColumn(inputCol, \n",
    "                             F.when(df[inputCol] == '', 'missing features')\n",
    "                             .otherwise(df[inputCol]))\n",
    "    #split df on \",\" and \"*\" stores as new data frame\n",
    "    feat_df = df.withColumn(\"features_list\", F.split(df[inputCol], ',| \\* '))\n",
    "    #explodes the features into column \"ex_features_list\"\n",
    "    feat_df_ex = feat_df.withColumn(\"ex_features_list\", F.explode(feat_df[\"features_list\"]))\n",
    "    #creates clustering data frame with only column \"ex_features_list\"\n",
    "    clustering_df = feat_df_ex[[\"ex_features_list\"]]\n",
    "    #renames the column\n",
    "    clustering_df = clustering_df.withColumnRenamed(\"ex_features_list\", \"text\")\n",
    "\n",
    "    #creates a tokenizer \n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    #removes stop words\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "    #hashes the features into sparse vectors\n",
    "    hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "    #invers document frequency - importance of the work (kind of)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "    \n",
    "    #creates and fits the pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "    pipelined_df = pipeline.fit(clustering_df).transform(clustering_df)\n",
    "    \n",
    "    #Set the number of clusters determined in rentalPrice_jonas.ipynb\n",
    "    num_k = 20\n",
    "    #creates the k-means\n",
    "    km = BisectingKMeans(k = num_k)\n",
    "    #fits it to the pipelined data frame\n",
    "    model = km.fit(pipelined_df)\n",
    "    #transform into the results\n",
    "    results = model.transform(pipelined_df)\n",
    "    #changes the name of the column \"prediction\" to \"cluster\"\n",
    "    results = results.withColumnRenamed(\"prediction\", \"clusters\")\n",
    "    \n",
    "    return results[\"clusters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = get_features_column(train_data_df, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manager work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def get_manager_skill_column(df, inputColum = \"manager_id\"):\n",
    "    string_indexer = StringIndexer(inputCol = inputColum, outputCol = \"manager_idx\")\n",
    "    manager_df = string_indexer.fit(df).transform(df)\n",
    "    \n",
    "    manager_join = manager_df[[\"manager_idx\"]].groupBy(\"manager_idx\").count()\n",
    "    \n",
    "    manager_df = manager_df.join(manager_join, on = \"manager_idx\", how = \"left\")\n",
    "    \n",
    "    manager_df = manager_df.withColumn(\"interest_level\", F.when(manager_df[\"interest_level\"] == 'low', 0)\n",
    "                                         .when(manager_df[\"interest_level\"] == 'medium', 1)\n",
    "                                         .otherwise(2))\n",
    "    \n",
    "    manager_skill = manager_df.groupBy(\"manager_idx\").agg({\"interest_level\": \"mean\"})\n",
    "    manager_skill = manager_skill.withColumnRenamed(\"avg(interest_level)\", \"manager_skill\")\n",
    "    \n",
    "    return manager_skill[\"manager_skill\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-71fa713fa84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmanager_skill_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_manager_skill_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"manager_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b2928ac27b50>\u001b[0m in \u001b[0;36mget_manager_skill_column\u001b[0;34m(df, inputColum)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_manager_skill_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputColum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"manager_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstring_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputColum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"manager_idx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmanager_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring_indexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmanager_join\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"manager_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"manager_idx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "manager_skill_column = get_manager_skill_column(new_df, \"manager_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magager_skill(train_df, test_df):\n",
    "    \n",
    "    #drops the column manager skill if it exists.\n",
    "    if \"manager_skill\" in test_df.columns:\n",
    "        test_df = test_df.drop(\"manager_skill\")\n",
    "    \n",
    "    #Calculates the average manager skill\n",
    "    avg_skill = train_df.select(F.mean(train_df['manager_skill'])).collect()[0][0]\n",
    "    \n",
    "    #Takes everey unique manager\n",
    "    temp_df = train_df.dropDuplicates([\"manager_id\"])[[\"manager_id\", \"manager_skill\"]]\n",
    "    \n",
    "    test_df = test_df.join(temp_df, on = \"manager_id\", how = \"left\")\n",
    "    test_df = test_df.na.fill(avg_skill)\n",
    "    \n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Make the replace function\n",
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(\"none\"))\n",
    "\n",
    "# Make the full function\n",
    "def add_description_columns(df):\n",
    "    # select only the description\n",
    "    #train_data_df2 = df.select(\"interest_level\",\"description\")\n",
    "    # clean blanks\n",
    "    train4 = df.withColumn(\"description\", replace(col(\"description\"), '        '))\n",
    "    train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"\"))\n",
    "    train4 = train4.withColumn(\"description\", replace(col(\"description\"), \" \"))\n",
    "    train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"           \"))\n",
    "    # regular expression tokenizer\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"words\", pattern=\"\\\\W\") # I don't know what W is...\n",
    "\n",
    "    # stop words\n",
    "    add_stopwords = [\"a\",\"the\",\"it\",\"of\",\"the\",\"is\",\"and\", # standard stop words\n",
    "     \"A\",\"this\",\"in\",\"for\"]\n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "    # bag of words count\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"word_features\", vocabSize=1000, minDF=5)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "    # Fit the pipeline to training documents.\n",
    "    pipelineFit = pipeline.fit(train4)\n",
    "    dataset = pipelineFit.transform(train4)\n",
    "    dataset = dataset.withColumn(\"label\", dataset[\"interest_level\"].cast(IntegerType()))\n",
    "    \n",
    "    return dataset[\"word_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_3 = add_description_columns(new_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features_col = add_description_columns(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'word_features'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the df where I made the label into integers\n",
    "def photo_taker(df):\n",
    "    train_data_df_p = df.select(\"interest_level\", \"photos\")\n",
    "    photo_rdd = train_data_df_p.rdd\n",
    "    photo_rdd = photo_rdd.map(lambda x: (x[0], len(x[1])))\n",
    "    photo_df = sqlCtx.createDataFrame(photo_rdd, [\"label\", \"features\"])\n",
    "    return photo_df[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'features'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_taker(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+--------------------+-------------------+--------------------+---------------+--------------------+--------+---------+--------------------+--------------------+-----+------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|manager_idx|listing_id|bathrooms|bedrooms|         building_id|            created|         description|display_address|            features|latitude|longitude|          manager_id|              photos|price|    street_address|interest_level|feature_cluster_0|feature_cluster_1|feature_cluster_2|feature_cluster_3|feature_cluster_4|feature_cluster_5|feature_cluster_6|feature_cluster_7|feature_cluster_8|feature_cluster_9|feature_cluster_10|feature_cluster_11|feature_cluster_12|feature_cluster_13|feature_cluster_14|feature_cluster_15|feature_cluster_16|feature_cluster_17|feature_cluster_18|feature_cluster_19|count|     manager_skill|               words|            filtered|       word_features|label|\n",
      "+-----------+----------+---------+--------+--------------------+-------------------+--------------------+---------------+--------------------+--------+---------+--------------------+--------------------+-----+------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      299.0|   7130943|      1.0|       3|ea11299b288bdb7e7...|2016-06-09 04:28:11|LOFT, 24h DOORMAN...| Park Ave South|doorman,elevator,...| 40.7399| -73.9864|21167417dbc081140...|[https://photos.r...| 5595|295 Park Ave South|             0|                1|                0|                0|                0|                0|                0|                0|                0|                1|                0|                 0|                 0|                 1|                 1|                 0|                 0|                 0|                 0|                 0|                 0|   36|0.5555555555555556|[loft, 24h, doorm...|[loft, 24h, doorm...|(1000,[0,1,2,5,6,...|    0|\n",
      "+-----------+----------+---------+--------+--------------------+-------------------+--------------------+---------------+--------------------+--------+---------+--------------------+--------------------+-----+------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----+------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same train/test setup as before\n",
    "dataset5_rdd = photo_rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "dataset5_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = sqlCtx.createDataFrame(dataset5_rdd, [\"label\", \"features\"])\n",
    "dataset5 = dataset5.withColumn(\"label\", dataset5[\"label\"].cast(IntegerType()))\n",
    "dataset5.printSchema()\n",
    "dataset5.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_num = [0, 2, 3, 4, 9, 10, 11, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_combine = ['manager_idx',\n",
    "                      'bathrooms',\n",
    "                      'bedrooms',\n",
    "                      'building_id',\n",
    "                      'latitude',\n",
    "                      'longitude',\n",
    "                      'manager_id',\n",
    "                      'price',\n",
    "                      'feature_cluster_0',\n",
    "                      'feature_cluster_1',\n",
    "                      'feature_cluster_2',\n",
    "                      'feature_cluster_3',\n",
    "                      'feature_cluster_4',\n",
    "                      'feature_cluster_5',\n",
    "                      'feature_cluster_6',\n",
    "                      'feature_cluster_7',\n",
    "                      'feature_cluster_8',\n",
    "                      'feature_cluster_9',\n",
    "                      'feature_cluster_10',\n",
    "                      'feature_cluster_11',\n",
    "                      'feature_cluster_12',\n",
    "                      'feature_cluster_13',\n",
    "                      'feature_cluster_14',\n",
    "                      'feature_cluster_15',\n",
    "                      'feature_cluster_16',\n",
    "                      'feature_cluster_17',\n",
    "                      'feature_cluster_18',\n",
    "                      'feature_cluster_19',\n",
    "                      'manager_skill'\n",
    "                      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[manager_idx: float, bathrooms: float, bedrooms: float, building_id: float, latitude: float, longitude: float, manager_id: float, price: float, feature_cluster_0: float, feature_cluster_1: float, feature_cluster_2: float, feature_cluster_3: float, feature_cluster_4: float, feature_cluster_5: float, feature_cluster_6: float, feature_cluster_7: float, feature_cluster_8: float, feature_cluster_9: float, feature_cluster_10: float, feature_cluster_11: float, feature_cluster_12: float, feature_cluster_13: float, feature_cluster_14: float, feature_cluster_15: float, feature_cluster_16: float, feature_cluster_17: float, feature_cluster_18: float, feature_cluster_19: float, manager_skill: float, word_features: vector, label: int]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_3b = new_data_3.select((*(col(c).cast(\"float\").alias(c) for c in features_to_combine)), \"word_features\",\"label\")\n",
    "new_data_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to rdd\n",
    "new_data_3_rdd = new_data_3b.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data_3_rdd.take(1)\n",
    "columns_num = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rdd = new_data_3_rdd.map(lambda x: (x[30], DenseVector([x[i] for i in columns_num])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  DenseVector([299.0, 1.0, 3.0, nan, 40.7399, -73.9864, nan, 5595.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5556])),\n",
       " (0,\n",
       "  DenseVector([299.0, 1.0, 2.0, nan, 40.7399, -73.9864, nan, 3995.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5556]))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rdd.take(2)\n",
    "\n",
    "# Ugh, need to convert word_features to a dense vector, unlist it, and then combine with this rdd I think...\n",
    "# We also have missing values..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

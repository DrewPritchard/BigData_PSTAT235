{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe assembler\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "* Input original dataset\n",
    "* Conduct feature engineering for the following columns:\n",
    "    * Lat/long: Add clusters, potentially; also neighborhood/other vars\n",
    "    * Features: Exploded? and k-means clustering into 20 clusters -- kapow!\n",
    "    * Manager: Add a manager score\n",
    "    * Description: replace with text analysis thing, add columns for exclamations and punctuation\n",
    "* This will generate a dataframe with several 'features' columns (eg. 'features_description', 'features_manager' etc.)\n",
    "* We will then combine these columns into a single column of features vectors:\n",
    "https://scikit-learn.org/0.18/auto_examples/hetero_feature_union.html looks very helpful for doing this\n",
    "\n",
    "* We then split the data using 20% testing, 80% cv with 5 folds of 16% to parameterize the model\n",
    "\n",
    "    * First model= logistic regression using no engineered features\n",
    "    * Second model= random forest with no engineered features\n",
    "\n",
    "    * Third model= logistic regression with engineered features\n",
    "    * Fourth model= random forest with engineered features\n",
    "\n",
    "\n",
    "Cross-validation and model comparison is based on log-loss.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Assemble_Data\") \\\n",
    "    .config(\"spark.executor.memory\", '4g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data_pd = pd.read_json(\"data/train.json\")\n",
    "train_data_df = sqlCtx.createDataFrame(train_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Lat/long work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Features' work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have commented the rows, for more thurough explanations look in `rentalPrice_jonas.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "def add_features_columns(df):\n",
    "    \n",
    "    #creates 1 sting of the features\n",
    "    string_assembler = F.UserDefinedFunction(lambda x: ','.join(x), typ.StringType())\n",
    "    df = df.withColumn(\"features\", string_assembler(df[\"features\"]))\n",
    "    #lower case everything\n",
    "    df = df.withColumn(\"features\", F.lower(df[\"features\"]))\n",
    "    #adds feature \"missing features\" to NaN\n",
    "    df = df.withColumn(\"features\", \n",
    "                             F.when(df[\"features\"] == '', 'missing features')\n",
    "                             .otherwise(df[\"features\"]))\n",
    "    #split df on \",\" and \"*\" stores as new data frame\n",
    "    feat_df = df.withColumn(\"features_list\", F.split(df[\"features\"], ',| \\* '))\n",
    "    #explodes the features into column \"ex_features_list\"\n",
    "    feat_df_ex = feat_df.withColumn(\"ex_features_list\", F.explode(feat_df[\"features_list\"]))\n",
    "    #creates clustering data frame with only column \"ex_features_list\"\n",
    "    clustering_df = feat_df_ex[[\"ex_features_list\"]]\n",
    "    #renames the column\n",
    "    clustering_df = clustering_df.withColumnRenamed(\"ex_features_list\", \"text\")\n",
    "\n",
    "    #creates a tokenizer \n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    #removes stop words\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "    #hashes the features into sparse vectors\n",
    "    hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "    #invers document frequency - importance of the work (kind of)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "    \n",
    "    #creates and fits the pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "    pipelined_df = pipeline.fit(clustering_df).transform(clustering_df)\n",
    "    \n",
    "    #Set the number of clusters determined in rentalPrice_jonas.ipynb\n",
    "    num_k = 20\n",
    "    #creates the k-means\n",
    "    km = BisectingKMeans(k = num_k)\n",
    "    #fits it to the pipelined data frame\n",
    "    model = km.fit(pipelined_df)\n",
    "    #transform into the results\n",
    "    results = model.transform(pipelined_df)\n",
    "    #changes the name of the column \"prediction\" to \"cluster\"\n",
    "    results = results.withColumnRenamed(\"prediction\", \"clusters\")\n",
    "    #drops the unnecessary columns\n",
    "    join_df = results.drop(*[\"tokens\", \"stopWordsRemovedTokens\", \"rawFeatures\", \"features\"])\n",
    "    #creates a column to add on\n",
    "    join_df = join_df.withColumn(\"join_col\", F.monotonically_increasing_id())\n",
    "    feat_df_ex = feat_df_ex.withColumn(\"join_col\", F.monotonically_increasing_id())\n",
    "    #joins the df_together\n",
    "    joined_df = feat_df_ex.join(join_df, feat_df_ex[\"join_col\"] == join_df[\"join_col\"], how = \"left\")\n",
    "    joined_df = joined_df.drop(\"join_col\")\n",
    "    #have to ad constatnt column for the pivot function \n",
    "    joined_df = joined_df.withColumn(\"constant_val\", F.lit(1))\n",
    "    #pivots the data frame\n",
    "    df_piv = joined_df\\\n",
    "                   .groupBy(\"listing_id\")\\\n",
    "                   .pivot(\"clusters\")\\\n",
    "                   .agg(F.coalesce(F.first(\"constant_val\")))\n",
    "    #Joins the data frame to the original\n",
    "    return_df = df.join(df_piv, on = \"listing_id\", how = \"left\")\n",
    "    #store the colusters in list, removes \"listing_id\"\n",
    "    cluster_col = df_piv.columns\n",
    "    cluster_col.remove(\"listing_id\")\n",
    "    #fills missing values\n",
    "    return_df = return_df.fillna(0, subset = cluster_col)\n",
    "    #changes the names of the columns to \"#_feature_cluster\" to the stings\n",
    "    for cluster in cluster_col:\n",
    "        return_df = return_df.withColumnRenamed(cluster, \"feature_cluster_\" + cluster)\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'listing_id',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = add_features_columns(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listing_id',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level',\n",
       " 'feature_cluster_0',\n",
       " 'feature_cluster_1',\n",
       " 'feature_cluster_2',\n",
       " 'feature_cluster_3',\n",
       " 'feature_cluster_4',\n",
       " 'feature_cluster_5',\n",
       " 'feature_cluster_6',\n",
       " 'feature_cluster_7',\n",
       " 'feature_cluster_8',\n",
       " 'feature_cluster_9',\n",
       " 'feature_cluster_10',\n",
       " 'feature_cluster_11',\n",
       " 'feature_cluster_12',\n",
       " 'feature_cluster_13',\n",
       " 'feature_cluster_14',\n",
       " 'feature_cluster_15',\n",
       " 'feature_cluster_16',\n",
       " 'feature_cluster_17',\n",
       " 'feature_cluster_18',\n",
       " 'feature_cluster_19']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manager work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def add_manager_skill(df):\n",
    "    string_indexer = StringIndexer(inputCol = \"manager_id\", outputCol = \"manager_idx\")\n",
    "    manager_df = string_indexer.fit(df).transform(df)\n",
    "    \n",
    "    manager_join = manager_df[[\"manager_idx\"]].groupBy(\"manager_idx\").count()\n",
    "    \n",
    "    manager_df = manager_df.join(manager_join, on = \"manager_idx\", how = \"left\")\n",
    "    \n",
    "    manager_df = manager_df.withColumn(\"interest_level\", F.when(manager_df[\"interest_level\"] == 'low', 0)\n",
    "                                         .when(manager_df[\"interest_level\"] == 'medium', 1)\n",
    "                                         .otherwise(2))\n",
    "    \n",
    "    manager_skill = manager_df.groupBy(\"manager_idx\").agg({\"interest_level\": \"mean\"})\n",
    "    manager_skill = manager_skill.withColumnRenamed(\"avg(interest_level)\", \"manager_skill\")\n",
    "    manager_df = manager_df.join(manager_skill, on = \"manager_idx\", how = \"left\")\n",
    "    \n",
    "    return manager_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'listing_id',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = add_manager_skill(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+-------------------+--------------------+----------------+--------------------+--------+---------+--------------------+--------------------+-----+--------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|listing_id|bathrooms|bedrooms|         building_id|            created|         description| display_address|            features|latitude|longitude|          manager_id|              photos|price|      street_address|interest_level|feature_cluster_0|feature_cluster_1|feature_cluster_2|feature_cluster_3|feature_cluster_4|feature_cluster_5|feature_cluster_6|feature_cluster_7|feature_cluster_8|feature_cluster_9|feature_cluster_10|feature_cluster_11|feature_cluster_12|feature_cluster_13|feature_cluster_14|feature_cluster_15|feature_cluster_16|feature_cluster_17|feature_cluster_18|feature_cluster_19|\n",
      "+----------+---------+--------+--------------------+-------------------+--------------------+----------------+--------------------+--------+---------+--------------------+--------------------+-----+--------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   6813946|      1.0|       0|e84a788c193121f20...|2016-04-02 02:50:04|Renovated studio ...|     W 44 Street|roof deck,elevato...| 40.7609| -73.9941|624c1fbd75e5f99e6...|[https://photos.r...| 1995|     461 W 44 Street|           low|                0|                0|                1|                0|                0|                1|                1|                0|                1|                0|                 0|                 0|                 1|                 1|                 0|                 1|                 0|                 0|                 0|                 0|\n",
      "|   6816559|      1.0|       3|dd7b281c0dbafc8b5...|2016-04-02 06:02:48|Luxurious and spa...|        2nd Ave.|roof deck,doorman...| 40.7413| -73.9779|66ad3cc67d71f41d5...|[https://photos.r...| 3900|        520 2nd Ave.|           low|                1|                0|                0|                0|                1|                0|                0|                0|                1|                0|                 0|                 0|                 1|                 1|                 1|                 1|                 0|                 0|                 0|                 1|\n",
      "|   6817923|      1.0|       3|2c8699a6cf54f0377...|2016-04-03 02:11:38|This unit comfort...|West 37th Street|pre-war,dogs allo...|  40.755| -73.9946|d2e027bcef0030787...|[https://photos.r...| 5450|356 West 37th Street|           low|                0|                0|                1|                0|                0|                1|                0|                1|                0|                0|                 0|                 0|                 0|                 0|                 0|                 0|                 0|                 0|                 0|                 0|\n",
      "+----------+---------+--------+--------------------+-------------------+--------------------+----------------+--------------------+--------+---------+--------------------+--------------------+-----+--------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magager_skill(train_df, test_df):\n",
    "    \n",
    "    #drops the column manager skill if it exists.\n",
    "    if \"manager_skill\" in test_df.columns:\n",
    "        test_df = test_df.drop(\"manager_skill\")\n",
    "    \n",
    "    #Calculates the average manager skill\n",
    "    avg_skill = train_df.select(F.mean(train_df['manager_skill'])).collect()[0][0]\n",
    "    \n",
    "    #Takes everey unique manager\n",
    "    temp_df = train_df.dropDuplicates([\"manager_id\"])[[\"manager_id\", \"manager_skill\"]]\n",
    "    \n",
    "    test_df = test_df.join(temp_df, on = \"manager_id\", how = \"left\")\n",
    "    test_df = test_df.na.fill(avg_skill)\n",
    "    \n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Make the replace function\n",
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(\"none\"))\n",
    "\n",
    "# Make the full function\n",
    "def add_description_columns(df):\n",
    "    # select only the description\n",
    "    #train_data_df2 = df.select(\"interest_level\",\"description\")\n",
    "    # clean blanks\n",
    "    train4 = df.withColumn(\"description\", replace(col(\"description\"), '        '))\n",
    "    train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"\"))\n",
    "    train4 = train4.withColumn(\"description\", replace(col(\"description\"), \" \"))\n",
    "    train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"           \"))\n",
    "    # regular expression tokenizer\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"words\", pattern=\"\\\\W\") # I don't know what W is...\n",
    "\n",
    "    # stop words\n",
    "    add_stopwords = [\"a\",\"the\",\"it\",\"of\",\"the\",\"is\",\"and\", # standard stop words\n",
    "     \"A\",\"this\",\"in\",\"for\"]\n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "    # bag of words count\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"word_features\", vocabSize=1000, minDF=5)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "    # Fit the pipeline to training documents.\n",
    "    pipelineFit = pipeline.fit(train4)\n",
    "    dataset = pipelineFit.transform(train4)\n",
    "    dataset = dataset.withColumn(\"label\", dataset[\"interest_level\"].cast(IntegerType()))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_3 = add_description_columns(new_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+-------------------+--------------------+-------------------+--------------------+--------+----------+---------+--------------------+--------------------+-----+--------------------+--------------+--------------------+--------------------+--------------------+-----+\n",
      "|bathrooms|bedrooms|         building_id|            created|         description|    display_address|            features|latitude|listing_id|longitude|          manager_id|              photos|price|      street_address|interest_level|               words|            filtered|       word_features|label|\n",
      "+---------+--------+--------------------+-------------------+--------------------+-------------------+--------------------+--------+----------+---------+--------------------+--------------------+-----+--------------------+--------------+--------------------+--------------------+--------------------+-----+\n",
      "|      1.5|       3|53a5b119ba8f7b61d...|2016-06-24 07:54:24|A Brand New 3 Bed...|Metropolitan Avenue|                  []| 40.7145|   7211212| -73.9425|5ba989232d0489da1...|[https://photos.r...| 3000|792 Metropolitan ...|        medium|[a, brand, new, 3...|[brand, new, 3, b...|(10000,[0,1,3,5,6...| null|\n",
      "|      1.0|       2|c5c8a357cba207596...|2016-06-12 12:19:27|                none|    Columbus Avenue|[Doorman, Elevato...| 40.7947|   7150865| -73.9667|7533621a882f71e25...|[https://photos.r...| 5465| 808 Columbus Avenue|           low|              [none]|              [none]| (10000,[246],[1.0])| null|\n",
      "|      1.0|       1|c3ba40552e2120b0a...|2016-04-17 03:26:41|Top Top West Vill...|        W 13 Street|[Laundry In Build...| 40.7388|   6887163| -74.0018|d9039c43983f6e564...|[https://photos.r...| 2850|     241 W 13 Street|          high|[top, top, west, ...|[top, top, west, ...|(10000,[0,1,2,3,4...| null|\n",
      "+---------+--------+--------------------+-------------------+--------------------+-------------------+--------------------+--------+----------+---------+--------------------+--------------------+-----+--------------------+--------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('manager', add_manager_skill), \n",
    "        ('description', add_description_columns) \n",
    "    ])),\n",
    "])\n",
    "\n",
    "# Ugh this is not working..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_3 = new_data_3.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+--------------------+-------------------+--------------------+---------------+--------------------+--------+---------+--------------------+--------------------+-----+------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|manager_idx|listing_id|bathrooms|bedrooms|         building_id|            created|         description|display_address|            features|latitude|longitude|          manager_id|              photos|price|    street_address|interest_level|feature_cluster_0|feature_cluster_1|feature_cluster_2|feature_cluster_3|feature_cluster_4|feature_cluster_5|feature_cluster_6|feature_cluster_7|feature_cluster_8|feature_cluster_9|feature_cluster_10|feature_cluster_11|feature_cluster_12|feature_cluster_13|feature_cluster_14|feature_cluster_15|feature_cluster_16|feature_cluster_17|feature_cluster_18|feature_cluster_19|count|     manager_skill|               words|            filtered|       word_features|label|\n",
      "+-----------+----------+---------+--------+--------------------+-------------------+--------------------+---------------+--------------------+--------+---------+--------------------+--------------------+-----+------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      299.0|   7130943|      1.0|       3|ea11299b288bdb7e7...|2016-06-09 04:28:11|LOFT, 24h DOORMAN...| Park Ave South|doorman,elevator,...| 40.7399| -73.9864|21167417dbc081140...|[https://photos.r...| 5595|295 Park Ave South|             0|                1|                0|                0|                0|                0|                0|                0|                0|                1|                0|                 0|                 0|                 1|                 1|                 0|                 0|                 0|                 0|                 0|                 0|   36|0.5555555555555556|[loft, 24h, doorm...|[loft, 24h, doorm...|(1000,[0,1,2,5,6,...|    0|\n",
      "+-----------+----------+---------+--------+--------------------+-------------------+--------------------+---------------+--------------------+--------+---------+--------------------+--------------------+-----+------------------+--------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----+------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data_3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_num = [0, 2, 3, 4, 9, 10, 11, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_combine = ['manager_idx',\n",
    "                      'bathrooms',\n",
    "                      'bedrooms',\n",
    "                      'building_id',\n",
    "                      'latitude',\n",
    "                      'longitude',\n",
    "                      'manager_id',\n",
    "                      'price',\n",
    "                      'feature_cluster_0',\n",
    "                      'feature_cluster_1',\n",
    "                      'feature_cluster_2',\n",
    "                      'feature_cluster_3',\n",
    "                      'feature_cluster_4',\n",
    "                      'feature_cluster_5',\n",
    "                      'feature_cluster_6',\n",
    "                      'feature_cluster_7',\n",
    "                      'feature_cluster_8',\n",
    "                      'feature_cluster_9',\n",
    "                      'feature_cluster_10',\n",
    "                      'feature_cluster_11',\n",
    "                      'feature_cluster_12',\n",
    "                      'feature_cluster_13',\n",
    "                      'feature_cluster_14',\n",
    "                      'feature_cluster_15',\n",
    "                      'feature_cluster_16',\n",
    "                      'feature_cluster_17',\n",
    "                      'feature_cluster_18',\n",
    "                      'feature_cluster_19',\n",
    "                      'manager_skill'\n",
    "                      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[manager_idx: float, bathrooms: float, bedrooms: float, building_id: float, latitude: float, longitude: float, manager_id: float, price: float, feature_cluster_0: float, feature_cluster_1: float, feature_cluster_2: float, feature_cluster_3: float, feature_cluster_4: float, feature_cluster_5: float, feature_cluster_6: float, feature_cluster_7: float, feature_cluster_8: float, feature_cluster_9: float, feature_cluster_10: float, feature_cluster_11: float, feature_cluster_12: float, feature_cluster_13: float, feature_cluster_14: float, feature_cluster_15: float, feature_cluster_16: float, feature_cluster_17: float, feature_cluster_18: float, feature_cluster_19: float, manager_skill: float, word_features: vector, label: int]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_3b = new_data_3.select((*(col(c).cast(\"float\").alias(c) for c in features_to_combine)), \"word_features\",\"label\")\n",
    "new_data_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to rdd\n",
    "new_data_3_rdd = new_data_3b.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data_3_rdd.take(1)\n",
    "columns_num = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rdd = new_data_3_rdd.map(lambda x: (x[30], DenseVector([x[i] for i in columns_num])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  DenseVector([299.0, 1.0, 3.0, nan, 40.7399, -73.9864, nan, 5595.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5556])),\n",
       " (0,\n",
       "  DenseVector([299.0, 1.0, 2.0, nan, 40.7399, -73.9864, nan, 3995.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5556]))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rdd.take(2)\n",
    "\n",
    "# Ugh, need to convert word_features to a dense vector, unlist it, and then combine with this rdd I think...\n",
    "# We also have missing values..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

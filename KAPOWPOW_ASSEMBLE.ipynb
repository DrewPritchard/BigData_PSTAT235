{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe assembler\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "* Input original dataset\n",
    "* Conduct feature engineering for the following columns:\n",
    "    * Lat/long: Add clusters, potentially; also neighborhood/other vars\n",
    "    * Features: Exploded? and k-means clustering into 20 clusters -- kapow!\n",
    "    * Manager: Add a manager score\n",
    "    * Description: replace with text analysis thing, add columns for exclamations and punctuation\n",
    "* This will generate a dataframe with several 'features' columns (eg. 'features_description', 'features_manager' etc.)\n",
    "* We will then combine these columns into a single column of features vectors:\n",
    "https://scikit-learn.org/0.18/auto_examples/hetero_feature_union.html looks very helpful for doing this\n",
    "\n",
    "* We then split the data using 20% testing, 80% cv with 5 folds of 16% to parameterize the model\n",
    "\n",
    "    * First model= logistic regression using no engineered features\n",
    "    * Second model= random forest with no engineered features\n",
    "\n",
    "    * Third model= logistic regression with engineered features\n",
    "    * Fourth model= random forest with engineered features\n",
    "\n",
    "\n",
    "Cross-validation and model comparison is based on log-loss.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Assemble_Data\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data_pd = pd.read_json(\"data/train.json\")\n",
    "train_data_df = sqlCtx.createDataFrame(train_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Lat/long work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Features' work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manager work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|interest_level|count|\n",
      "+--------------+-----+\n",
      "|           low|34284|\n",
      "|          high| 3839|\n",
      "|        medium|11229|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### I am assuming that I will take a df that has already coded 'interest_level' as 0, 1, or 2. I also assume that no records have been removed.\n",
    "### Note: we should go through dataframe naming conventions!\n",
    "\n",
    "# select only the description\n",
    "train_data_df2 = train_data_df.select(\"interest_level\",\"description\")\n",
    "# check it works\n",
    "train_data_df2.groupBy(\"interest_level\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|interest_level|         description|\n",
      "+--------------+--------------------+\n",
      "|        medium|A Brand New 3 Bed...|\n",
      "|           low|                none|\n",
      "+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deal with missing values\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(\"none\"))\n",
    "\n",
    "train4 = train_data_df2.withColumn(\"description\", replace(col(\"description\"), '        '))\n",
    "train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"\"))\n",
    "train4 = train4.withColumn(\"description\", replace(col(\"description\"), \" \"))\n",
    "train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"           \"))\n",
    "\n",
    "\n",
    "train4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use a count vectorizer for this part. For the presentation, I'll simply show that we compared several methods\n",
    "# (count vectorizer, hashing, word2vec) and used only the training set to decide which one would be better.\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"words\", pattern=\"\\\\W\") # I don't know what W is...\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"a\",\"the\",\"it\",\"of\",\"the\",\"is\",\"and\", # standard stop words\n",
    "     \"A\",\"this\",\"in\",\"for\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|interest_level|         description|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|        medium|A Brand New 3 Bed...|[a, brand, new, 3...|[brand, new, 3, b...|(10000,[0,1,3,5,6...| null|\n",
      "|           low|                none|              [none]|              [none]| (10000,[246],[1.0])| null|\n",
      "|          high|Top Top West Vill...|[top, top, west, ...|[top, top, west, ...|(10000,[0,1,2,3,4...| null|\n",
      "|           low|Building Amenitie...|[building, amenit...|[building, amenit...|(10000,[0,1,2,3,4...| null|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(train4)\n",
    "dataset = pipelineFit.transform(train4)\n",
    "dataset = dataset.withColumn(\"label\", dataset[\"interest_level\"].cast(IntegerType()))\n",
    "dataset.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we select the features column to combine with the ones created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

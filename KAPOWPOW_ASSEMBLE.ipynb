{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe assembler\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "* Input original dataset\n",
    "* Conduct feature engineering for the following columns:\n",
    "    * Lat/long: Add clusters, potentially; also neighborhood/other vars\n",
    "    * Features: Exploded? and k-means clustering into 20 clusters -- kapow!\n",
    "    * Manager: Add a manager score\n",
    "    * Description: replace with text analysis thing, add columns for exclamations and punctuation\n",
    "* This will generate a dataframe with several 'features' columns (eg. 'features_description', 'features_manager' etc.)\n",
    "* We will then combine these columns into a single column of features vectors:\n",
    "https://scikit-learn.org/0.18/auto_examples/hetero_feature_union.html looks very helpful for doing this\n",
    "\n",
    "* We then split the data using 20% testing, 80% cv with 5 folds of 16% to parameterize the model\n",
    "\n",
    "    * First model= logistic regression using no engineered features\n",
    "    * Second model= random forest with no engineered features\n",
    "\n",
    "    * Third model= logistic regression with engineered features\n",
    "    * Fourth model= random forest with engineered features\n",
    "\n",
    "\n",
    "Cross-validation and model comparison is based on log-loss.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Assemble_Data\") \\\n",
    "    .config(\"spark.executor.memory\", '4g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data_pd = pd.read_json(\"data/train.json\")\n",
    "train_data_df = sqlCtx.createDataFrame(train_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Lat/long work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Features' work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have commented the rows, for more thurough explanations look in `rentalPrice_jonas.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "def add_features_columns(df):\n",
    "    \n",
    "    #creates 1 sting of the features\n",
    "    string_assembler = F.UserDefinedFunction(lambda x: ','.join(x), typ.StringType())\n",
    "    df = df.withColumn(\"features\", string_assembler(df[\"features\"]))\n",
    "    #lower case everything\n",
    "    df = df.withColumn(\"features\", F.lower(df[\"features\"]))\n",
    "    #adds feature \"missing features\" to NaN\n",
    "    df = df.withColumn(\"features\", \n",
    "                             F.when(df[\"features\"] == '', 'missing features')\n",
    "                             .otherwise(df[\"features\"]))\n",
    "    #split df on \",\" and \"*\" stores as new data frame\n",
    "    feat_df = df.withColumn(\"features_list\", F.split(df[\"features\"], ',| \\* '))\n",
    "    #explodes the features into column \"ex_features_list\"\n",
    "    feat_df_ex = feat_df.withColumn(\"ex_features_list\", F.explode(feat_df[\"features_list\"]))\n",
    "    #creates clustering data frame with only column \"ex_features_list\"\n",
    "    clustering_df = feat_df_ex[[\"ex_features_list\"]]\n",
    "    #renames the column\n",
    "    clustering_df = clustering_df.withColumnRenamed(\"ex_features_list\", \"text\")\n",
    "\n",
    "    #creates a tokenizer \n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    #removes stop words\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "    #hashes the features into sparse vectors\n",
    "    hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "    #invers document frequency - importance of the work (kind of)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "    \n",
    "    #creates and fits the pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "    pipelined_df = pipeline.fit(clustering_df).transform(clustering_df)\n",
    "    \n",
    "    #Set the number of clusters determined in rentalPrice_jonas.ipynb\n",
    "    num_k = 20\n",
    "    #creates the k-means\n",
    "    km = BisectingKMeans(k = num_k)\n",
    "    #fits it to the pipelined data frame\n",
    "    model = km.fit(pipelined_df)\n",
    "    #transform into the results\n",
    "    results = model.transform(pipelined_df)\n",
    "    #changes the name of the column \"prediction\" to \"cluster\"\n",
    "    results = results.withColumnRenamed(\"prediction\", \"clusters\")\n",
    "    #drops the unnecessary columns\n",
    "    join_df = results.drop(*[\"tokens\", \"stopWordsRemovedTokens\", \"rawFeatures\", \"features\"])\n",
    "    #creates a column to add on\n",
    "    join_df = join_df.withColumn(\"join_col\", F.monotonically_increasing_id())\n",
    "    feat_df_ex = feat_df_ex.withColumn(\"join_col\", F.monotonically_increasing_id())\n",
    "    #joins the df_together\n",
    "    joined_df = feat_df_ex.join(join_df, feat_df_ex[\"join_col\"] == join_df[\"join_col\"], how = \"left\")\n",
    "    joined_df = joined_df.drop(\"join_col\")\n",
    "    #have to ad constatnt column for the pivot function \n",
    "    joined_df = joined_df.withColumn(\"constant_val\", F.lit(1))\n",
    "    #pivots the data frame\n",
    "    df_piv = joined_df\\\n",
    "                   .groupBy(\"listing_id\")\\\n",
    "                   .pivot(\"clusters\")\\\n",
    "                   .agg(F.coalesce(F.first(\"constant_val\")))\n",
    "    #Joins the data frame to the original\n",
    "    return_df = df.join(df_piv, on = \"listing_id\", how = \"left\")\n",
    "    #store the colusters in list, removes \"listing_id\"\n",
    "    cluster_col = df_piv.columns\n",
    "    cluster_col.remove(\"listing_id\")\n",
    "    #fills missing values\n",
    "    return_df = return_df.fillna(0, subset = cluster_col)\n",
    "    #changes the names of the columns to \"#_feature_cluster\" to the stings\n",
    "    for cluster in cluster_col:\n",
    "        return_df = return_df.withColumnRenamed(cluster, \"feature_cluster_\" + cluster)\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'listing_id',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = add_features_columns(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listing_id',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level',\n",
       " 'feature_cluster_0',\n",
       " 'feature_cluster_1',\n",
       " 'feature_cluster_2',\n",
       " 'feature_cluster_3',\n",
       " 'feature_cluster_4',\n",
       " 'feature_cluster_5',\n",
       " 'feature_cluster_6',\n",
       " 'feature_cluster_7',\n",
       " 'feature_cluster_8',\n",
       " 'feature_cluster_9',\n",
       " 'feature_cluster_10',\n",
       " 'feature_cluster_11',\n",
       " 'feature_cluster_12',\n",
       " 'feature_cluster_13',\n",
       " 'feature_cluster_14',\n",
       " 'feature_cluster_15',\n",
       " 'feature_cluster_16',\n",
       " 'feature_cluster_17',\n",
       " 'feature_cluster_18',\n",
       " 'feature_cluster_19']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manager work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def add_manager_skill(df):\n",
    "    string_indexer = StringIndexer(inputCol = \"manager_id\", outputCol = \"manager_idx\")\n",
    "    manager_df = string_indexer.fit(df).transform(df)\n",
    "    \n",
    "    manager_join = manager_df[[\"manager_idx\"]].groupBy(\"manager_idx\").count()\n",
    "    \n",
    "    manager_df = manager_df.join(manager_join, on = \"manager_idx\", how = \"left\")\n",
    "    \n",
    "    manager_df = manager_df.withColumn(\"interest_level\", F.when(manager_df[\"interest_level\"] == 'low', 0)\n",
    "                                         .when(manager_df[\"interest_level\"] == 'medium', 1)\n",
    "                                         .otherwise(2))\n",
    "    \n",
    "    manager_skill = manager_df.groupBy(\"manager_idx\").agg({\"interest_level\": \"mean\"})\n",
    "    manager_skill = manager_skill.withColumnRenamed(\"avg(interest_level)\", \"manager_skill\")\n",
    "    manager_df = manager_df.join(manager_skill, on = \"manager_idx\", how = \"left\")\n",
    "    \n",
    "    return manager_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'listing_id',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = add_manager_skill(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manager_idx',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'building_id',\n",
       " 'created',\n",
       " 'description',\n",
       " 'display_address',\n",
       " 'features',\n",
       " 'latitude',\n",
       " 'listing_id',\n",
       " 'longitude',\n",
       " 'manager_id',\n",
       " 'photos',\n",
       " 'price',\n",
       " 'street_address',\n",
       " 'interest_level',\n",
       " 'count',\n",
       " 'manager_skill']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magager_skill(train_df, test_df):\n",
    "    \n",
    "    #drops the column manager skill if it exists.\n",
    "    if \"manager_skill\" in test_df.columns:\n",
    "        test_df = test_df.drop(\"manager_skill\")\n",
    "    \n",
    "    #Calculates the average manager skill\n",
    "    avg_skill = train_df.select(F.mean(train_df['manager_skill'])).collect()[0][0]\n",
    "    \n",
    "    #Takes everey unique manager\n",
    "    temp_df = train_df.dropDuplicates([\"manager_id\"])[[\"manager_id\", \"manager_skill\"]]\n",
    "    \n",
    "    test_df = test_df.join(temp_df, on = \"manager_id\", how = \"left\")\n",
    "    test_df = test_df.na.fill(avg_skill)\n",
    "    \n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|interest_level|count|\n",
      "+--------------+-----+\n",
      "|           low|34284|\n",
      "|          high| 3839|\n",
      "|        medium|11229|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### I am assuming that I will take a df that has already coded 'interest_level' as 0, 1, or 2. I also assume that no records have been removed.\n",
    "### Note: we should go through dataframe naming conventions!\n",
    "\n",
    "# select only the description\n",
    "train_data_df2 = train_data_df.select(\"interest_level\",\"description\")\n",
    "# check it works\n",
    "train_data_df2.groupBy(\"interest_level\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|interest_level|         description|\n",
      "+--------------+--------------------+\n",
      "|        medium|A Brand New 3 Bed...|\n",
      "|           low|                none|\n",
      "+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deal with missing values\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(\"none\"))\n",
    "\n",
    "train4 = train_data_df2.withColumn(\"description\", replace(col(\"description\"), '        '))\n",
    "train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"\"))\n",
    "train4 = train4.withColumn(\"description\", replace(col(\"description\"), \" \"))\n",
    "train4 = train4.withColumn(\"description\", replace(col(\"description\"), \"           \"))\n",
    "\n",
    "\n",
    "train4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use a count vectorizer for this part. For the presentation, I'll simply show that we compared several methods\n",
    "# (count vectorizer, hashing, word2vec) and used only the training set to decide which one would be better.\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"words\", pattern=\"\\\\W\") # I don't know what W is...\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"a\",\"the\",\"it\",\"of\",\"the\",\"is\",\"and\", # standard stop words\n",
    "     \"A\",\"this\",\"in\",\"for\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|interest_level|         description|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|        medium|A Brand New 3 Bed...|[a, brand, new, 3...|[brand, new, 3, b...|(10000,[0,1,3,5,6...| null|\n",
      "|           low|                none|              [none]|              [none]| (10000,[246],[1.0])| null|\n",
      "|          high|Top Top West Vill...|[top, top, west, ...|[top, top, west, ...|(10000,[0,1,2,3,4...| null|\n",
      "|           low|Building Amenitie...|[building, amenit...|[building, amenit...|(10000,[0,1,2,3,4...| null|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(train4)\n",
    "dataset = pipelineFit.transform(train4)\n",
    "dataset = dataset.withColumn(\"label\", dataset[\"interest_level\"].cast(IntegerType()))\n",
    "dataset.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we select the features column to combine with the ones created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
